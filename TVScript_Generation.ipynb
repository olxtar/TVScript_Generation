{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7075fc7",
   "metadata": {},
   "source": [
    "# TV Script Generation\n",
    "\n",
    "Seinfeld : 미국 시트콤\n",
    "\n",
    "이번 프로젝트에서는 Seinfeld라는 미국시트콤의 시즌 9의 대사(Script)를 데이터로 하여서 \n",
    "\n",
    "그와 유사한 대사를 생성하는 모델을 만들 것 이다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1c0166",
   "metadata": {},
   "source": [
    "## 00. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86095b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\USER\\\\Desktop\\\\Udacity\\\\TVScript_Generation\\\\code'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0d834fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b497f312",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1f601f",
   "metadata": {},
   "source": [
    "## 01. Get the Data\n",
    "\n",
    "Seinfeld script는 `\\TVScript_Generation\\data` 폴더내에 있다\n",
    "\n",
    "<span style=\"color:red\">[!]</span> 경로에서 '.'은 현재 디렉토리를 의미하고, '..'는 상위 디렉토리를 의미한다\n",
    "\n",
    "따라서 `./../` 은 현재 디렉토리에서의 상위디렉토리, 즉 TVScript_Generation 폴더를 의미한다\n",
    "\n",
    "<span style=\"color:red\">[!]</span> Udacity에서 다운로드한 Seinfeld_Scripts.txt를 열라고하니 cpc949 문제가 일어났다\n",
    "\n",
    "따라서 직접 메모장으로 ANSI로 변환 저장하여 Seinfeld_Scripts_1.txt로 오픈함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b6eb70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper.py의 load_data 함수를 이용하여 데이터를 불러오자\n",
    "\n",
    "import helper\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Load Dataset from File\n",
    "    \"\"\"\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, \"r\", encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e384a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './../data/Seinfeld_Scripts_1.txt'\n",
    "text = helper.load_data(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ef661d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3471464"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "52a2b344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6260baa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1884f19a",
   "metadata": {},
   "source": [
    "## 02. Explore the Data\n",
    "\n",
    "Seinfeld 스크립트 텍스트 데이터는 모두 소문자이고, 각 대화는 `\\n`으로 구분되어 있다.\n",
    "\n",
    "이와 같은 특성들을 데이터를 직접보면서 확인할 수 있어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f8528d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트데이터 길이 : 3471464\n",
      "The number of unique word : 46367\n",
      "The number of Lines : 109233\n"
     ]
    }
   ],
   "source": [
    "print(f\"텍스트데이터 길이 : {len(text)}\")\n",
    "print(f\"The number of unique word : {len(set([word for word in text.split()]))}\")\n",
    "lines = text.split('\\n')\n",
    "print(f\"The number of Lines : {len(lines)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a6ec0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jerry: oh, you dont recall? \n",
      "\n",
      "george: (on an imaginary microphone) uh, no, not at this time. \n",
      "\n",
      "jerry: well, senator, id just like to know, what you knew and when you knew it. \n",
      "\n",
      "claire: mr. seinfeld. mr. costanza. \n",
      "\n",
      "george: are, are you sure this is decaf? wheres the orange indicator? \n",
      "\n",
      "claire: its missing, i have to do it in my head decaf left, regular right, decaf left, regular right...its very challenging work. \n",
      "\n",
      "jerry: can you relax, its a cup of coffee. claire is a professional waitress. \n",
      "\n",
      "claire: trust me george. no one has any interest in seeing you on caffeine. \n",
      "\n",
      "george: how come youre not doing the second show tomorrow? \n",
      "\n",
      "jerry: well, theres this uh, woman might be coming in. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 대사의 라인 살펴보기\n",
    "\n",
    "print('\\n'.join(text.split('\\n')[10:30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14fb039",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a4b42a",
   "metadata": {},
   "source": [
    "## 03. Pre-processing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e898983",
   "metadata": {},
   "source": [
    "### 3-1. Lookup Table\n",
    "\n",
    "* input : `text`; words로 나눠진 TV scripts text data\n",
    "* return : A tuple of dictionaries (`vocab_to_int`, `int_to_vocab`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "38a2c4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9bf43ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_tables(text):\n",
    "    counts = Counter(text)\n",
    "    vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "    \n",
    "    vocab_to_int = {word: ii for ii, word in enumerate(vocab)}\n",
    "    int_to_vocab = {ii: word for word, ii in vocab_to_int.items()}\n",
    "    \n",
    "    return (vocab_to_int, int_to_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7f0ba13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "# Udacity test function을 통한 create_lookup_tables함수 검증\n",
    "\n",
    "import problem_unittests as tests\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc7340c",
   "metadata": {},
   "source": [
    "### 3-2. Tokenize Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab72c89e",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">[!]</span> 구두점 : Token 로 이루어진 Dictionary를 만들어서 추후에 각 구두점(기호)을 하나의 단어처럼 만들고, 공백을 통해 구분할 수 있게끔 할 것임\n",
    "\n",
    "<span style=\"color:skyblue\">ex)</span> {'!' : '||Exclamation_mark||'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "67a88ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenized dictionary where the key is the punctuation and the value is the token\n",
    "    구두점 : Token으로 이루어진 Dictionary 반환\n",
    "    \"\"\"\n",
    "        \n",
    "    return {\n",
    "        '.' : '||Period||',\n",
    "        ',' : '||Comma||',\n",
    "        '\"' : '||Quotation_Mark||',\n",
    "        ';' : '||Semicolon||',\n",
    "        '!' : '||Exclamation_mark||',\n",
    "        '?' : '||Question_mark||',\n",
    "        '(' : '||Left_Parentheses||',\n",
    "        ')' : '||Right_Parentheses||',\n",
    "        '-' : '||Dash||',\n",
    "        '\\n' : '||Return||'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d3d984c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "tests.test_tokenize(token_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faebc86",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2760d489",
   "metadata": {},
   "source": [
    "## 04. Pre-process the data and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "369d751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper.py의 preprocess_and_save_data 함수를 통해서 \n",
    "# 전처리된 텍스트데이터를 pickle로 저장하자\n",
    "\n",
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dea1fc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_WORDS = {'PADDING': '<PAD>'}\n",
    "\n",
    "def preprocess_and_save_data(dataset_path, token_lookup, create_lookup_tables):\n",
    "    \"\"\"\n",
    "    Preprocess Text Data\n",
    "    \"\"\"\n",
    "    text = load_data(dataset_path)\n",
    "    \n",
    "    # Ignore notice, since we don't use it for analysing the data\n",
    "    # 앞부분은 버리네...?\n",
    "    text = text[81:]\n",
    "\n",
    "    # token_dict에 앞서만든 구두점 딕셔너리 할당\n",
    "    # for loop로 text데이터의 구두점들을 모두 Token으로 replace\n",
    "    token_dict = token_lookup()          \n",
    "    for key, token in token_dict.items():\n",
    "        text = text.replace(key, ' {} '.format(token))\n",
    "\n",
    "    \n",
    "    # 모두 소문자로 변경\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 단어 단위로 split\n",
    "    text = text.split()\n",
    "\n",
    "    vocab_to_int, int_to_vocab = create_lookup_tables(text + list(SPECIAL_WORDS.values()))\n",
    "    int_text = [vocab_to_int[word] for word in text]\n",
    "    pickle.dump((int_text, vocab_to_int, int_to_vocab, token_dict), open('preprocess.p', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f820a403",
   "metadata": {},
   "source": [
    "* `dataset_path`를 받아서 앞부분은 버리네?\n",
    "* 만들어두었던 구두점 토큰화 딕셔너리를 통해서 텍스트데이터내의 구두점들을 모두 ||Punctuation|| 이렇게 바꿈\n",
    "* 소문자로 변경, 그리고 단어 단위로 쪼개기\n",
    "* (패딩 Token도 추가해서) 단어<->정수 딕셔너리x2 만들기\n",
    "* `int_text`에 단어-> 정수 변환시켜서 담아넣기\n",
    "---\n",
    "* pickle로 아래의 것들 저장\n",
    "   * `int_text` : 단어, 구두점 -> 정수화 된 리스트\n",
    "   * `vocab_to_int` : 단어->정수 딕셔너리\n",
    "   * `int_to_vocab` : 정수->단어 딕셔너리\n",
    "   * `token_dict` : 구두점->Token 딕셔너리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bbf900e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 pickle파일 (preprocess.p) 불러와보기\n",
    "import pickle\n",
    "\n",
    "with open('preprocess.p', 'rb') as file:\n",
    "    preprocessed_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fd0b8bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_text = preprocessed_data[0]\n",
    "vocab_to_int = preprocessed_data[1]\n",
    "int_to_vocab = preprocessed_data[2]\n",
    "token_dict = preprocessed_data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "379492cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24, 22, 47, 1, 1, 1, 17, 47, 22, 82]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정수로 변환된 단어 토큰리스트\n",
    "int_text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "55c17c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 -> 정수 딕셔너리\n",
    "vocab_to_int[\"||period||\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c5bee78d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'||period||'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정수 -> 단어 딕셔너리\n",
    "int_to_vocab[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "326ad314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': '||Period||',\n",
       " ',': '||Comma||',\n",
       " '\"': '||Quotation_Mark||',\n",
       " ';': '||Semicolon||',\n",
       " '!': '||Exclamation_mark||',\n",
       " '?': '||Question_mark||',\n",
       " '(': '||Left_Parentheses||',\n",
       " ')': '||Right_Parentheses||',\n",
       " '-': '||Dash||',\n",
       " '\\n': '||Return||'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 구두점 -> Token 딕셔너리\n",
    "token_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dfe881",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "\n",
    "코드스크립트를 재시작할때 이 부분으로부터의 윗부분은 다시 실행할 필요 없게끔\n",
    "\n",
    "체크포인트 생성\n",
    "\n",
    "위에서 저장한 pickle에 전처리된 데이터들이 있으므로 그거만 로드해주자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "52c03f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "import problem_unittests as test\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "34f0cdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocess():\n",
    "    \"\"\"\n",
    "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
    "    \"\"\"\n",
    "    return pickle.load(open('preprocess.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f51e19",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79442516",
   "metadata": {},
   "source": [
    "## 05. Build the Neural Network\n",
    "\n",
    "RNN 모듈과 forward and backpropagation function을 만들것이다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec4f79d",
   "metadata": {},
   "source": [
    "### 5-0. Check Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "901acb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found. Please use a GPU to train your neural network\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print(\"No GPU found. Please use a GPU to train your neural network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8768a5b1",
   "metadata": {},
   "source": [
    "### 5-1. Input\n",
    "\n",
    "이제 우리는 아래와 같은 전처리가 된 '데이터'를 가지고 있다.\n",
    "\n",
    "* 구두점들 Tokenize 완료 <span style=\"color:skyblue\">ex)</span> `.` $\\rightarrow$ `||Period||`\n",
    "* 모든 Character 소문자로변경\n",
    "* 단어단위로 split\n",
    "* 빈도수순으로 넘버링하여 단어 $\\rightarrow$ Token(정수)\n",
    "\n",
    "이제 단순히 892,132개(8십9만2천1백2십3개)의 정수로 이루어진 리스트, 즉 `int_text`\n",
    "\n",
    "전처리된 텍스트 데이터를 Neural Network가 편하게 받아들일 수 있게 `DataLoader`로 만들어줘야한다\n",
    "\n",
    "(batch, shuffle, iteration 등을 쉽게 할 수 있음)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a91dd50",
   "metadata": {},
   "source": [
    "#### Batching\n",
    "\n",
    "우리가 가진 `int_text`, 즉 일렬로 나열된 Tokenize된 단어 리스트\n",
    "\n",
    "이는 Seinfeld TV 시트콤 스크립트를 단순히 나열한 것이다\n",
    "\n",
    "우리의 목적인 TV Script Generation을 하러면 \n",
    "\n",
    "앞의 단어 몇개를 먹였을때, 그 다음 단어로 예측되는 단어를 뱉으면 되는거잖아?\n",
    "\n",
    "따라서 sequence length개의 단어, $x$ 데이터와 바로 그 다음 단어인 $y$ 데이터를\n",
    "\n",
    "하나의 Batch로 만들어줄 필요가 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281a04a8",
   "metadata": {},
   "source": [
    "<span style=\"color:skyblue\">ex)</span> 주어진 단어리스트가 `words`, sequence length가 `seq_length` 이고\n",
    "\n",
    "$x$, 즉 입력데이터를 `feature_tensor`, $y$, 즉 정답데이터를 `target_tensor`라고 하면\n",
    "\n",
    "`words = [1, 2, 3, 4, 5, 6, 7]`\n",
    "\n",
    "`seq_length = 4`\n",
    "\n",
    "---\n",
    "\n",
    "1) `feature_tensor = [1, 2, 3, 4]`\n",
    "`target_tensor = 5`\n",
    "\n",
    "2) `feature_tensor = [2, 3, 4, 5]`\n",
    "`target_tensor = 6`\n",
    "\n",
    "3) `feature_tensor = [3, 4, 5, 6]`\n",
    "`target_tensor = 7`\n",
    "\n",
    "이런식으로 3개의 Batch화 시킬 수 있어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d24eb9",
   "metadata": {},
   "source": [
    "#### Tips 1.\n",
    "\n",
    "어떤 길이 $n$의 리스트 데이터가 있다. 그 데이터를 앞에서부터 $m$개씩 꺼내서 사용(윈도우, 한번 꺼낸 이후에는 다음 순서부터 꺼내기 시작)하면 총 몇번 꺼내서 사용할 수 있을까? \n",
    "\n",
    "$\\rightarrow n - m + 1$\n",
    "\n",
    "<span style=\"color:skyblue\">ex)</span> $n$ : [1,2,3,4,5,6,7], $m$ : 3이라면\n",
    "\n",
    "[1,2,3], [2,3,4], [3,4,5], [4,5,6], [5,6,7] 총 5개"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e530b677",
   "metadata": {},
   "source": [
    "#### Tips 2.\n",
    "\n",
    "어떤 길이 $n$의 리스트 데이터가 있다. 그 데이터를 앞에서부터 $m$개씩 꺼내서 사용하고 $m+1$번째 데이터를 짝으로써 사용한다면, 총 몇 세트의 데이터를 만들 수 있을까?\n",
    "\n",
    "$\\rightarrow n - m + 1 - 1 = n - m$\n",
    "\n",
    "<span style=\"color:skyblue\">ex)</span> $n$ : [1,2,3,4,5,6,7], $m$ : 3이라면\n",
    "\n",
    "* [1,2,3], 4\n",
    "* [2,3,4], 5\n",
    "* [3,4,5], 6\n",
    "* [4,5,6], 7\n",
    "\n",
    "총 4세트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a71813c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d81a13",
   "metadata": {},
   "source": [
    "#### Practice\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "578abee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "t_text = [1,2,3,4,5,6,7]\n",
    "seq_length = 3\n",
    "n_batches = len(t_text) - seq_length\n",
    "print(n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fb917b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (n_batches, seq_length) size의 x데이터, 즉 0 feature_tensors 생성\n",
    "# n_batches size의 y데이터, 즉 0 target_tensors 생성\n",
    "\n",
    "feature_tensors = np.zeros( (n_batches, seq_length), dtype=int)\n",
    "target_tensors = np.zeros( (n_batches), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ba11b86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "\n",
      "[0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(feature_tensors)\n",
    "print()\n",
    "print(target_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6e3b7de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0로만 이루어진 x,y에 데이터를 삽입\n",
    "\n",
    "for i in range(0, n_batches):\n",
    "    feature_tensors[i] = t_text[i:i+seq_length]\n",
    "    target_tensors[i] = t_text[ (i+seq_length) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d660ad5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [2, 3, 4],\n",
       "       [3, 4, 5],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "01557aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 5, 6, 7])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b58bd2e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bc909e",
   "metadata": {},
   "source": [
    "### 5-2. Batching & Make DataLoader function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7141c0",
   "metadata": {},
   "source": [
    "입력으로 아래의 parameter들을 넣으면 알아서 DataLoader를 반환해주는 함수를 만들자\n",
    "\n",
    "* `words` : 단어(정수Token)들이 일렬로 나열된 리스트\n",
    "* `sequence_length` : 하나의 mini batch의 x데이터에 들어갈 단어(Token) 개수\n",
    "* `batch_size` : 하나의 (큰) batch에 들어갈 mini batch의 개수, sequence의 개수\n",
    "\n",
    "<span style=\"color:green\">[+]</span> Batch vs mini-batch\n",
    "\n",
    "1. 일렬로 나열된 단어 리스트 <span style=\"color:skyblue\">ex)</span> [1,2,3,4,5,6,7]\n",
    "2. 윈도우 크기, 즉 sequence length개의 $x$와 바로 다음의 $y$로 쪼개서 mini-batch만듦\n",
    "<span style=\"color:skyblue\">ex)</span> [1,2,3] 4 / [2,3,4] 5 / [3,4,5] 6 / [4,5,6] 7\n",
    "3. 이렇게 만들어진 mini-batch들을 하나의 (큰) Batch에 몇개씩 담느냐가 `batch_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e2824d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def batch_data(words, sequence_length, batch_size):\n",
    "    \"\"\"\n",
    "    words : The word token(id) of the TV scripts\n",
    "    sequence_length : The sequence length of 각 (mini)batch\n",
    "    batch_size : 하나의 batch에 mini batch가 몇개들어가는지, 즉 sequence가 몇개인지\n",
    "    return : DataLoader with batched data\n",
    "    \"\"\"\n",
    "    \n",
    "    n_batches = len(words) - sequence_length\n",
    "    feature_tensors = np.zeros( (n_batches, sequence_length), dtype=int)\n",
    "    target_tensors = np.zeros( (n_batches), dtype=int)\n",
    "    \n",
    "    for i in range(0, n_batches):\n",
    "        feature_tensors[i] = words[i:i+sequence_length]\n",
    "        target_tensors[i] = words[ (i+sequence_length) ]\n",
    "        \n",
    "    dataset = TensorDataset(torch.from_numpy(feature_tensors),\n",
    "                            torch.from_numpy(target_tensors)\n",
    "                           )\n",
    "    \n",
    "    dataloader = DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c07eac2",
   "metadata": {},
   "source": [
    "### 5-3. Batching & Make DataLoader function Test\n",
    "\n",
    "0부터 49까지의(총 50개) 리스트를 만들어서 $\\leftarrow$ `words`\n",
    "\n",
    "sequence length = 5, (큰) Batch에 들어가는 mini-batch 개수, 즉 `batch_size`를 10으로 하는 Batch를 뱉는 DataLoader를 만들어보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e04f95c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5])\n",
      "tensor([[ 2,  3,  4,  5,  6],\n",
      "        [29, 30, 31, 32, 33],\n",
      "        [24, 25, 26, 27, 28],\n",
      "        [ 7,  8,  9, 10, 11],\n",
      "        [17, 18, 19, 20, 21],\n",
      "        [33, 34, 35, 36, 37],\n",
      "        [16, 17, 18, 19, 20],\n",
      "        [35, 36, 37, 38, 39],\n",
      "        [30, 31, 32, 33, 34],\n",
      "        [ 5,  6,  7,  8,  9]], dtype=torch.int32)\n",
      "\n",
      "torch.Size([10])\n",
      "tensor([ 7, 34, 29, 12, 22, 38, 21, 40, 35, 10], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# test dataloader\n",
    "\n",
    "test_text = range(50)\n",
    "t_loader = batch_data(test_text, sequence_length=5, batch_size=10)\n",
    "\n",
    "data_iter = iter(t_loader)\n",
    "sample_x, sample_y = data_iter.next()\n",
    "\n",
    "print(sample_x.shape)\n",
    "print(sample_x)\n",
    "print()\n",
    "print(sample_y.shape)\n",
    "print(sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19a4fb8",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">[!]</span> \n",
    "\n",
    "* input 리스트 길이, $n$ : 50\n",
    "* sequence length, $m$ : 5 이므로\n",
    "\n",
    "총 45개의 ($x$,$y$) 데이터셋, 즉 mini-batch를 만들 수 있으나\n",
    "\n",
    "그 45개의 mini-batch중에서 10개씩(`batch_size`)만 묶어서 Batch로 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d2c4f39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini batch의 개수 : 45\n",
      "Batch의 개수 : 5\n"
     ]
    }
   ],
   "source": [
    "print(f\"mini batch의 개수 : {len(t_loader.dataset)}\")\n",
    "print(f\"Batch의 개수 : {len(t_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "edf8e291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[16, 17, 18, 19, 20],\n",
      "        [33, 34, 35, 36, 37],\n",
      "        [43, 44, 45, 46, 47],\n",
      "        [ 8,  9, 10, 11, 12],\n",
      "        [41, 42, 43, 44, 45],\n",
      "        [ 6,  7,  8,  9, 10],\n",
      "        [24, 25, 26, 27, 28],\n",
      "        [10, 11, 12, 13, 14],\n",
      "        [32, 33, 34, 35, 36],\n",
      "        [ 9, 10, 11, 12, 13]], dtype=torch.int32), tensor([21, 38, 48, 13, 46, 11, 29, 15, 37, 14], dtype=torch.int32)]\n",
      "[tensor([[38, 39, 40, 41, 42],\n",
      "        [44, 45, 46, 47, 48],\n",
      "        [29, 30, 31, 32, 33],\n",
      "        [28, 29, 30, 31, 32],\n",
      "        [35, 36, 37, 38, 39],\n",
      "        [15, 16, 17, 18, 19],\n",
      "        [19, 20, 21, 22, 23],\n",
      "        [17, 18, 19, 20, 21],\n",
      "        [14, 15, 16, 17, 18],\n",
      "        [12, 13, 14, 15, 16]], dtype=torch.int32), tensor([43, 49, 34, 33, 40, 20, 24, 22, 19, 17], dtype=torch.int32)]\n",
      "[tensor([[18, 19, 20, 21, 22],\n",
      "        [20, 21, 22, 23, 24],\n",
      "        [ 0,  1,  2,  3,  4],\n",
      "        [22, 23, 24, 25, 26],\n",
      "        [ 3,  4,  5,  6,  7],\n",
      "        [36, 37, 38, 39, 40],\n",
      "        [ 2,  3,  4,  5,  6],\n",
      "        [37, 38, 39, 40, 41],\n",
      "        [27, 28, 29, 30, 31],\n",
      "        [23, 24, 25, 26, 27]], dtype=torch.int32), tensor([23, 25,  5, 27,  8, 41,  7, 42, 32, 28], dtype=torch.int32)]\n",
      "[tensor([[21, 22, 23, 24, 25],\n",
      "        [31, 32, 33, 34, 35],\n",
      "        [ 5,  6,  7,  8,  9],\n",
      "        [39, 40, 41, 42, 43],\n",
      "        [ 1,  2,  3,  4,  5],\n",
      "        [26, 27, 28, 29, 30],\n",
      "        [ 4,  5,  6,  7,  8],\n",
      "        [42, 43, 44, 45, 46],\n",
      "        [13, 14, 15, 16, 17],\n",
      "        [11, 12, 13, 14, 15]], dtype=torch.int32), tensor([26, 36, 10, 44,  6, 31,  9, 47, 18, 16], dtype=torch.int32)]\n",
      "[tensor([[25, 26, 27, 28, 29],\n",
      "        [30, 31, 32, 33, 34],\n",
      "        [40, 41, 42, 43, 44],\n",
      "        [34, 35, 36, 37, 38],\n",
      "        [ 7,  8,  9, 10, 11]], dtype=torch.int32), tensor([30, 35, 45, 39, 12], dtype=torch.int32)]\n"
     ]
    }
   ],
   "source": [
    "for d in t_loader:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e135559",
   "metadata": {},
   "source": [
    "### 5-4. Build the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbfc130",
   "metadata": {},
   "source": [
    "PyTorch의 Module class를 통하여 RNN을 구현하자. GRU or LSTM을 사용할 수 있고 \n",
    "아래와 같은 Function이 구현되어야 한다\n",
    "\n",
    "* `__init__` : 초기화 함수\n",
    "* `init_hidden` : LSTM/GRU의 Hidden state에 대한 초기화 함수\n",
    "* `forward` : Forward propagation 함수\n",
    "\n",
    "---\n",
    "\n",
    "`__init__` 함수는 network의 layer를 생성하여 class에 저장해야 한다.\n",
    "\n",
    "`forward` 함수는 이러한 Layer들을 사용하여 Forward propagation을 실행하고 output, hidden state를 생성한다\n",
    "\n",
    "이 모델의 output은 모든 sequence가 처리된 후의 (마지막 sequence에 대한) score만 출력한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9942da3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "821b0f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocab_size,     # Unique Vocab 개수, 즉 Embedding 직전의 단어당 unit수\n",
    "                 output_size,    # Unique Vocab 개수, 예측된 단어 Score의 개수\n",
    "                 embedding_dim,  # Embedding layer로 차원축소시킨 dimension\n",
    "                 hidden_dim,     # RNN의 cell내 unit수\n",
    "                 n_layers,\n",
    "                 dropout=0.5):\n",
    "        \n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # set class variables\n",
    "        self.n_layers = n_layers\n",
    "        self.output_size = output_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Define model layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # + LSTM(input_size, hidden_dim, n_layers, dropout, batch_first)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)      # x : (batch_size, seq_length, input_size) from DataLoader\n",
    "        x = x.long()                # 정수형 Tensor로 변환\n",
    "        embeds = self.embedding(x)  # embeds : (batch_size, seq_length, embedding_dim)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "                                    # lstm_out : (batch_size, seq_length, hidden_dim)\n",
    "        # Stack-up\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "                                    # lstm_out : (batch_size*seq_length, hidden_dim)\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)          # out : (batch_size*seq_length, output_size)\n",
    "        out = out.view(batch_size, -1, self.output_size)\n",
    "                                    # out : (batch_size, seq_length, output_size)\n",
    "        out = out[:, -1]\n",
    "                                    # out : (batch_size, 마지막, output_size)\n",
    "            \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if train_on_gpu:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "            \n",
    "        return hidden\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "189ed688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "tests.test_rnn(RNN,train_on_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37131db6",
   "metadata": {},
   "source": [
    "### 5-5. Define forward and backpropagation\n",
    "\n",
    "위에서 만든 RNN 클래스를 이용하여 Forward propagation / Backward propagation를 진행할\n",
    "함수를 만들자\n",
    "\n",
    "기능들\n",
    "\n",
    "* data를 GPU로 move(GPU available일때)\n",
    "* return : loss, hidden\n",
    "\n",
    "---\n",
    "\n",
    "Parameters\n",
    "\n",
    "* `decoder` : The PyTorch module that holds the NN, 즉 선언한 모델 변수\n",
    "* `decoder_optimizer` : The PyTorch optimizer for NN\n",
    "* `criterion` : The PyTorch loss function\n",
    "* `inp` : NN에 입력될 Input Batch\n",
    "* `target` : input Batch의 target, 즉 정답\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef74928",
   "metadata": {},
   "source": [
    "#### Trouble Shooting \n",
    "\n",
    "\n",
    "```\n",
    "RuntimeError: Expected object of scalar type Long but got scalar type int when using CrossEntropyLoss\n",
    "```\n",
    "\n",
    "음... 정확히는 모르겠지만 CrossEntropyLoss -> Criterion, 즉 Loss function을 사용할때\n",
    "\n",
    "criterion(x데이터, y데이터) 이렇게 사용하잖아? \n",
    "\n",
    "* `x` : 보통 score? 확률? 따라서 float\n",
    "* `y` : 정수, LongTensor여야 하나봐\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "38bb0c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input의 data type : torch.int32\n",
      "target,label의 data type : torch.int32\n"
     ]
    }
   ],
   "source": [
    "x,y = next(iter(train_loader))\n",
    "print(f\"input의 data type : {x.dtype}\")\n",
    "print(f\"target,label의 data type : {y.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "5c2b1619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output의 data type : torch.float32\n"
     ]
    }
   ],
   "source": [
    "h = rnn.init_hidden(100)\n",
    "output, h = rnn(x,h)\n",
    "print(f\"output의 data type : {output.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda9de95",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b8ca6236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_back_prop(rnn, optimizer, criterion, inputs, targets, hidden):\n",
    "    \n",
    "    # Move data to GPU, if available\n",
    "    if train_on_gpu:\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        \n",
    "    # Perform backpropagation and optimization\n",
    "    hidden = tuple([each.data for each in hidden])\n",
    "    \n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    output, hidden = rnn(inputs, hidden)\n",
    "    loss = criterion(output.squeeze(), targets.long())  # target, 즉 label, y 데이터도 LongTensor로 변경시켜주자!\n",
    "    loss.backward()\n",
    "    \n",
    "    nn.utils.clip_grad_norm_(rnn.parameters(), 5)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item(), hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "00d702a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "tests.test_forward_back_prop(RNN, forward_back_prop, train_on_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8f0d6a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fb1257",
   "metadata": {},
   "source": [
    "## 06. Neural Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0441e5d",
   "metadata": {},
   "source": [
    "### 6-1. Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "50a317bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "dc1842f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(rnn,\n",
    "               batch_size,\n",
    "               optimizer,\n",
    "               criterion,\n",
    "               n_epochs,\n",
    "               show_every_n_batches=100):\n",
    "    \n",
    "    batch_losses = []\n",
    "    \n",
    "    rnn.train()\n",
    "    \n",
    "    print(f\"Training for {n_epochs}...\")\n",
    "    for epoch_i in range(1, n_epochs + 1):    # epoch개수 (1부터)\n",
    "        start = time()\n",
    "        \n",
    "        # initialize hidden state\n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        \n",
    "        # batch_i : 몇번째 batch인지 (1부터)\n",
    "        # inputs, labels : DataLoader로 받은 x,y\n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            \n",
    "            # 꽉찬 Batch로만 Training\n",
    "            n_batches = len(train_loader.dataset) // batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "                \n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden )\n",
    "            \n",
    "            # 매 Batch마다의 loss값을 list에 쌓아두기\n",
    "            batch_losses.append(loss)\n",
    "            \n",
    "            # Loss stats 출력\n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                print(f\"Epoch: {epoch_i}/{n_epochs}  Loss: {np.average(batch_losses)}\")\n",
    "                # batch_losses 초기화\n",
    "                batch_losses = []\n",
    "                \n",
    "        end = time()\n",
    "        print(f\"epoch time spend: {end-start}[sec]\")\n",
    "        \n",
    "    return rnn\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff9ffa5",
   "metadata": {},
   "source": [
    "### 6-2. Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8a7abe",
   "metadata": {},
   "source": [
    "* `sequence_length` : Sequence length (RNN으로 누적해서 몇개의 단어까지 볼건지)\n",
    "* `batch_size` : 하나의 Batch에 몇개의 x,y 데이터셋을 넣을건지\n",
    "* `num_epochs` : Training epoch를 몇번할건지\n",
    "* `learning_rate` : Learning rate\n",
    "* `vocab_size` : Unique vocabulary 개수 (또는 모델의 input layer's unit수, 임베딩전)\n",
    "* `output_size` : = `vocab_size` (우리는 다음 단어를 예측하는 것이므로)\n",
    "* `embedding_dim` : Embedding layer로 몇차원까지 축소할지\n",
    "* `hidden_dim` : RNN cell내의 unit수\n",
    "* `n_layers` : RNN layer 개수\n",
    "* `show_every_n_batches` : 몇번의 Batch training 마다 loss(평균)을 출력할건지\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82326660",
   "metadata": {},
   "source": [
    "#### Data parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ac829e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 3  # of words in a sequence\n",
    "batch_size = 100\n",
    "\n",
    "train_loader = batch_data(int_text, sequence_length, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78473beb",
   "metadata": {},
   "source": [
    "#### Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "336d14dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0b0fc5",
   "metadata": {},
   "source": [
    "#### Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "6365624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab_to_int)     # = 21,383\n",
    "output_size = vocab_size\n",
    "embedding_dim = 400\n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "\n",
    "show_every_n_batches = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb58d4cc",
   "metadata": {},
   "source": [
    "### 6-3. Train\n",
    "\n",
    "<span style=\"color:red\">[!]</span> GPU가 없으므로 간단하게 epoch는 2로만 해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "90eaf7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import workspace_utils\n",
    "from workspace_utils import active_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "fc971b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(vocab_size,\n",
    "          output_size,\n",
    "          embedding_dim,\n",
    "          hidden_dim,\n",
    "          n_layers,\n",
    "          dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "6bf1b5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1...\n",
      "Epoch: 1/1  Loss: 5.48026679611206\n",
      "Epoch: 1/1  Loss: 5.350816349983216\n",
      "Epoch: 1/1  Loss: 5.25140052318573\n",
      "Epoch: 1/1  Loss: 5.262663858413696\n",
      "Epoch: 1/1  Loss: 5.202031468391419\n",
      "Epoch: 1/1  Loss: 5.191882739067077\n",
      "Epoch: 1/1  Loss: 5.1273360452651975\n",
      "Epoch: 1/1  Loss: 5.129231917381286\n",
      "Epoch: 1/1  Loss: 5.112112811088562\n",
      "Epoch: 1/1  Loss: 5.089050556659698\n",
      "Epoch: 1/1  Loss: 5.1027843661308285\n",
      "Epoch: 1/1  Loss: 5.067692330360413\n",
      "Epoch: 1/1  Loss: 5.080345338344574\n",
      "Epoch: 1/1  Loss: 5.100670188903808\n",
      "Epoch: 1/1  Loss: 5.077177362442017\n",
      "Epoch: 1/1  Loss: 5.061386499404907\n",
      "Epoch: 1/1  Loss: 5.108796964645386\n",
      "epoch time spend: 5265.764845371246[sec]\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "if train_on_gpu:\n",
    "    rnn.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# with active_session():\n",
    "trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n",
    "    \n",
    "    \n",
    "# Save the trained model\n",
    "helper.save_model('./save/trained_rnn_epoch1', trained_rnn)\n",
    "print(\"Model Trained and Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedf799f",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">[!]</span> model save error 관련 practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "bd984569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(filename, decoder):\n",
    "    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n",
    "    torch.save(decoder, save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "7dbbf528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(21383, 400)\n",
       "  (lstm): LSTM(400, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=512, out_features=21383, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "bde60742",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(rnn, 'save_test.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d850c1e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2acfa3",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d5136287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import helper\n",
    "import problem_unittests as test\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "trained_rnn = helper.load_model('./save/trained_rnn_epoch1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db198b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper.py의 load_model 함수\n",
    "# 단순히 입력된 filename에다가 .pt를 붙여서 불러옴\n",
    "def load_model(filename):\n",
    "    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n",
    "    return torch.load(save_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0056ec80",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d92ba16",
   "metadata": {},
   "source": [
    "## 07. Generate TV Script\n",
    "\n",
    "저장된 훈련완료 network를 통해서 새로운 ~~'가짜'~~ Seinfeld TV script를 만들어보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d364e320",
   "metadata": {},
   "source": [
    "'텍스트'를 생성하려면 network는 하나의 단어로 시작하고 사용자가 임의로 정해준 (예측)길이까지 예측을 반복한다\n",
    "\n",
    "* `prime_id` : 시작 단어 (integer)\n",
    "* `predict_len` : 예측하여 생성하고 싶은 단어 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "396be490",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n",
    "    \"\"\"\n",
    "    Generate text using the neural network\n",
    "    :param decoder: The PyTorch Module that holds the trained neural network\n",
    "    :param prime_id: The word id to start the first prediction\n",
    "    :param int_to_vocab: Dict of word id keys to word values\n",
    "    :param token_dict: Dict of puncuation tokens keys to puncuation values\n",
    "    :param pad_value: The value used to pad a sequence\n",
    "    :param predict_len: The length of text to generate\n",
    "    :return: The generated text\n",
    "    \"\"\"\n",
    "    rnn.eval()\n",
    "    \n",
    "    # create a sequence (batch_size=1) with the prime_id\n",
    "    current_seq = np.full((1, sequence_length), pad_value)   # (size)의 array를 pad_value로 채운다\n",
    "    current_seq[-1][-1] = prime_id\n",
    "    predicted = [int_to_vocab[prime_id]]\n",
    "    \n",
    "    for _ in range(predict_len):\n",
    "        if train_on_gpu:\n",
    "            current_seq = torch.LongTensor(current_seq).cuda()\n",
    "        else:\n",
    "            current_seq = torch.LongTensor(current_seq)\n",
    "        \n",
    "        # initialize the hidden state\n",
    "        hidden = rnn.init_hidden(current_seq.size(0))\n",
    "        \n",
    "        # get the output of the rnn\n",
    "        output, _ = rnn(current_seq, hidden)\n",
    "        \n",
    "        # get the next word probabilities\n",
    "        p = F.softmax(output, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "         \n",
    "        # use top_k sampling to get the index of the next word\n",
    "        top_k = 5\n",
    "        p, top_i = p.topk(top_k)\n",
    "        top_i = top_i.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next word index with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
    "        \n",
    "        # retrieve that word from the dictionary\n",
    "        word = int_to_vocab[word_i]\n",
    "        predicted.append(word)     \n",
    "        \n",
    "        # the generated word becomes the next \"current sequence\" and the cycle can continue\n",
    "        current_seq = np.roll(current_seq, -1, axis=1) # 축기준으로 왼쪽(-1)으로 요소들 이동\n",
    "        current_seq[-1][-1] = word_i\n",
    "    \n",
    "    gen_sentences = ' '.join(predicted)\n",
    "    \n",
    "    # Replace punctuation tokens\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        gen_sentences = gen_sentences.replace(' ' + token.lower(), key)\n",
    "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
    "    gen_sentences = gen_sentences.replace('( ', '(')\n",
    "    \n",
    "    # return all the sentences\n",
    "    return gen_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a25dc77",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4a9456",
   "metadata": {},
   "source": [
    "## 08. Generate a New Script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fd5d46",
   "metadata": {},
   "source": [
    "* `gen_length` : 새롭게 생성할 Script의 단어개수\n",
    "* `prime_word` : 처음에 넣어줄 prime 단어 `jerry`, `elaine`, `george` 등 추천"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "d0dc0717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jerry: you,. you, i just got...\n",
      "\n",
      "elaine: you know.\n",
      "\n",
      "george: i know what?\n",
      "\n",
      "george: yeah, no i just just, you?\n",
      "\n",
      "elaine: you........\n",
      "\n",
      "jerry: i know, i got you?\n",
      "\n",
      "kramer: you. i don't know. you?\n",
      "\n",
      "kramer: no.\n",
      "\n",
      "jerry: i got a.\n",
      "\n",
      "kramer: no.\n",
      "\n",
      "elaine: yeah.\n",
      "\n",
      "george: yeah. i don't want to be..\n",
      "\n",
      "george:(, the the\n",
      "\n",
      "jerry: i got the\n",
      "\n",
      "jerry:(,(.. you you, i got a.\n",
      "\n",
      "elaine: i just got.\n",
      "\n",
      "kramer: oh, i just just just, you.\n",
      "\n",
      "elaine: oh, i don't know, i know.\n",
      "\n",
      "elaine: i got you, i don't have a. i don't want me.\n",
      "\n",
      "jerry:(the?\n",
      "\n",
      "george: i have you, the. you, i don't want me to a-.\n",
      "\n",
      "george: oh yeah, i just was the\n",
      "\n",
      "kramer:(,.\n",
      "\n",
      "jerry:(the?\n",
      "\n",
      "elaine:(, you, you have to the, and, i don't want you know, i got you, you know what is it.....\n",
      "\n",
      "george:(the.\n",
      "\n",
      "kramer: oh!\n",
      "\n",
      "jerry: yeah, i have a, and.\n",
      "\n",
      "kramer: i know, you have to you?\n",
      "\n",
      "george:(,.\n",
      "\n",
      "jerry: i got a.\n",
      "\n",
      "elaine: i have to be, i got you.\n",
      "\n",
      "jerry: oh, i have a- the\n",
      "\n",
      "jerry: i don't know, the, you don't think i don't want to you, you don't think it's...\n",
      "\n",
      "jerry: i got.\n",
      "\n",
      "jerry: yeah! i don't want to.\n",
      "\n",
      "george:(.\n",
      "\n",
      "jerry:(, the the?\n",
      "\n",
      "elaine: oh, i know, i don't have to the. i just just know\n"
     ]
    }
   ],
   "source": [
    "# run the cell multiple times to get different results!\n",
    "gen_length = 400 # modify the length to your preference\n",
    "prime_word = 'jerry' # name for starting the script\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "pad_word = helper.SPECIAL_WORDS['PADDING']\n",
    "generated_script = generate(trained_rnn, vocab_to_int[prime_word + ':'], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
    "print(generated_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83001d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
